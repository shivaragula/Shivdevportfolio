# Machine Learning Model Deployment Platform

A platform for deploying, monitoring, and managing machine learning models in production with automated scaling and performance tracking.

## üöÄ Features

- **Automated Model Deployment**: One-click deployment with Docker containers
- **Real-time Monitoring**: Performance metrics and model drift detection
- **A/B Testing Framework**: Compare model versions in production
- **Model Versioning**: Track and manage different model versions
- **Auto-scaling**: Dynamic scaling based on request load
- **Performance Analytics**: Comprehensive model performance insights

## üõ†Ô∏è Tech Stack

- **Backend**: Python, FastAPI, Celery
- **Containerization**: Docker, Kubernetes
- **Database**: PostgreSQL, Redis
- **ML Frameworks**: TensorFlow, PyTorch, Scikit-learn
- **Monitoring**: Prometheus, Grafana
- **Message Queue**: RabbitMQ
- **Cloud**: AWS ECS, Google Cloud Run

## üìä Key Metrics

- 99.9% model uptime in production
- 50ms average inference latency
- 20+ models deployed simultaneously
- Auto-scaling up to 100 instances

## ü§ñ Model Deployment Pipeline

### Model Registration
```python
from mlflow import MLflow
import joblib
import docker

class ModelRegistry:
    def __init__(self):
        self.mlflow_client = MLflow()
        self.docker_client = docker.from_env()
    
    def register_model(self, model_path, model_name, version, metadata):
        """Register a new model version"""
        
        # Validate model
        model = joblib.load(model_path)
        validation_score = self.validate_model(model)
        
        if validation_score < 0.8:
            raise ValueError("Model validation failed")
        
        # Register in MLflow
        model_version = self.mlflow_client.create_model_version(
            name=model_name,
            source=model_path,
            run_id=metadata.get('run_id')
        )
        
        # Build Docker image
        image_tag = f"{model_name}:{version}"
        self.build_model_image(model_path, image_tag, metadata)
        
        return model_version
    
    def build_model_image(self, model_path, image_tag, metadata):
        """Build Docker image for model serving"""
        
        dockerfile_content = f"""
        FROM python:3.9-slim
        
        WORKDIR /app
        
        COPY requirements.txt .
        RUN pip install -r requirements.txt
        
        COPY {model_path} ./model.pkl
        COPY serve.py .
        
        EXPOSE 8000
        
        CMD ["python", "serve.py"]
        """
        
        # Build image
        self.docker_client.images.build(
            fileobj=dockerfile_content,
            tag=image_tag,
            rm=True
        )
```

### Model Serving API
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import joblib
import numpy as np
import logging
from prometheus_client import Counter, Histogram, generate_latest

app = FastAPI()

# Metrics
REQUEST_COUNT = Counter('model_requests_total', 'Total model requests')
REQUEST_LATENCY = Histogram('model_request_duration_seconds', 'Request latency')
PREDICTION_COUNT = Counter('model_predictions_total', 'Total predictions made')

class PredictionRequest(BaseModel):
    features: list
    model_version: str = "latest"

class ModelServer:
    def __init__(self):
        self.models = {}
        self.load_models()
    
    def load_models(self):
        """Load all available model versions"""
        # Load from model registry
        model_versions = self.get_available_versions()
        
        for version in model_versions:
            model_path = f"models/{version}/model.pkl"
            self.models[version] = joblib.load(model_path)
    
    @REQUEST_LATENCY.time()
    def predict(self, features, model_version="latest"):
        REQUEST_COUNT.inc()
        
        if model_version not in self.models:
            raise HTTPException(status_code=404, detail="Model version not found")
        
        model = self.models[model_version]
        
        try:
            # Preprocess features
            processed_features = self.preprocess_features(features)
            
            # Make prediction
            prediction = model.predict([processed_features])
            confidence = model.predict_proba([processed_features]).max()
            
            PREDICTION_COUNT.inc()
            
            # Log prediction for monitoring
            self.log_prediction(features, prediction, confidence, model_version)
            
            return {
                "prediction": prediction.tolist(),
                "confidence": float(confidence),
                "model_version": model_version
            }
            
        except Exception as e:
            logging.error(f"Prediction error: {str(e)}")
            raise HTTPException(status_code=500, detail="Prediction failed")

model_server = ModelServer()

@app.post("/predict")
async def predict(request: PredictionRequest):
    return model_server.predict(request.features, request.model_version)

@app.get("/health")
async def health_check():
    return {"status": "healthy", "models_loaded": len(model_server.models)}

@app.get("/metrics")
async def metrics():
    return generate_latest()
```

## üöÄ Getting Started

### Prerequisites
- Python 3.8+
- Docker and Docker Compose
- Kubernetes (optional)
- PostgreSQL 13+
- Redis 6+

### Installation

```bash
# Clone the repository
git clone https://github.com/shivaragula/ml-deployment-platform.git
cd ml-deployment-platform

# Set up virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set up infrastructure
docker-compose up -d postgres redis prometheus grafana

# Initialize database
python scripts/init_db.py

# Start the platform
python main.py
```

## üîß Configuration

### Environment Variables
```env
# Database
DATABASE_URL=postgresql://user:password@localhost:5432/ml_platform
REDIS_URL=redis://localhost:6379

# MLflow
MLFLOW_TRACKING_URI=http://localhost:5000
MLFLOW_S3_ENDPOINT_URL=http://localhost:9000

# Monitoring
PROMETHEUS_URL=http://localhost:9090
GRAFANA_URL=http://localhost:3000

# Kubernetes
KUBECONFIG_PATH=/path/to/kubeconfig
NAMESPACE=ml-platform

# Scaling
MIN_REPLICAS=1
MAX_REPLICAS=10
TARGET_CPU_UTILIZATION=70
```

## üìä Model Monitoring

### Drift Detection
```python
import numpy as np
from scipy import stats
from sklearn.metrics import accuracy_score

class ModelMonitor:
    def __init__(self, model_name):
        self.model_name = model_name
        self.baseline_stats = self.load_baseline_stats()
    
    def detect_data_drift(self, new_data, baseline_data):
        """Detect statistical drift in input data"""
        
        drift_scores = {}
        
        for feature_idx in range(new_data.shape[1]):
            # Kolmogorov-Smirnov test
            ks_stat, p_value = stats.ks_2samp(
                baseline_data[:, feature_idx],
                new_data[:, feature_idx]
            )
            
            drift_scores[f'feature_{feature_idx}'] = {
                'ks_statistic': ks_stat,
                'p_value': p_value,
                'drift_detected': p_value < 0.05
            }
        
        return drift_scores
    
    def detect_performance_drift(self, predictions, actual_labels):
        """Monitor model performance degradation"""
        
        current_accuracy = accuracy_score(actual_labels, predictions)
        baseline_accuracy = self.baseline_stats['accuracy']
        
        performance_drop = baseline_accuracy - current_accuracy
        
        return {
            'current_accuracy': current_accuracy,
            'baseline_accuracy': baseline_accuracy,
            'performance_drop': performance_drop,
            'significant_drop': performance_drop > 0.05
        }
    
    def generate_alert(self, drift_results, performance_results):
        """Generate alerts for significant drift or performance issues"""
        
        alerts = []
        
        # Data drift alerts
        for feature, results in drift_results.items():
            if results['drift_detected']:
                alerts.append({
                    'type': 'data_drift',
                    'feature': feature,
                    'severity': 'medium',
                    'message': f'Data drift detected in {feature}'
                })
        
        # Performance drift alerts
        if performance_results['significant_drop']:
            alerts.append({
                'type': 'performance_drift',
                'severity': 'high',
                'message': f'Model performance dropped by {performance_results["performance_drop"]:.2%}'
            })
        
        return alerts
```

## üîÑ A/B Testing Framework

### Experiment Management
```python
import random
from datetime import datetime, timedelta

class ABTestManager:
    def __init__(self):
        self.experiments = {}
    
    def create_experiment(self, name, model_a, model_b, traffic_split=0.5):
        """Create a new A/B test experiment"""
        
        experiment = {
            'name': name,
            'model_a': model_a,
            'model_b': model_b,
            'traffic_split': traffic_split,
            'start_time': datetime.now(),
            'metrics': {'a': [], 'b': []},
            'active': True
        }
        
        self.experiments[name] = experiment
        return experiment
    
    def route_request(self, experiment_name, user_id):
        """Route request to appropriate model version"""
        
        experiment = self.experiments.get(experiment_name)
        if not experiment or not experiment['active']:
            return experiment['model_a']  # Default to model A
        
        # Consistent routing based on user ID
        hash_value = hash(str(user_id)) % 100
        threshold = experiment['traffic_split'] * 100
        
        if hash_value < threshold:
            return experiment['model_a']
        else:
            return experiment['model_b']
    
    def record_metric(self, experiment_name, model_version, metric_value):
        """Record performance metric for experiment"""
        
        experiment = self.experiments.get(experiment_name)
        if experiment:
            variant = 'a' if model_version == experiment['model_a'] else 'b'
            experiment['metrics'][variant].append({
                'value': metric_value,
                'timestamp': datetime.now()
            })
    
    def analyze_experiment(self, experiment_name):
        """Analyze A/B test results"""
        
        experiment = self.experiments.get(experiment_name)
        if not experiment:
            return None
        
        metrics_a = [m['value'] for m in experiment['metrics']['a']]
        metrics_b = [m['value'] for m in experiment['metrics']['b']]
        
        if len(metrics_a) < 30 or len(metrics_b) < 30:
            return {'status': 'insufficient_data'}
        
        # Statistical significance test
        t_stat, p_value = stats.ttest_ind(metrics_a, metrics_b)
        
        return {
            'model_a_mean': np.mean(metrics_a),
            'model_b_mean': np.mean(metrics_b),
            'improvement': (np.mean(metrics_b) - np.mean(metrics_a)) / np.mean(metrics_a),
            'p_value': p_value,
            'significant': p_value < 0.05,
            'winner': 'b' if np.mean(metrics_b) > np.mean(metrics_a) and p_value < 0.05 else 'a'
        }
```

## ‚ò∏Ô∏è Kubernetes Deployment

### Model Deployment Manifest
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-model-server
  labels:
    app: ml-model-server
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ml-model-server
  template:
    metadata:
      labels:
        app: ml-model-server
    spec:
      containers:
      - name: model-server
        image: ml-platform/model-server:latest
        ports:
        - containerPort: 8000
        env:
        - name: MODEL_VERSION
          value: "v1.0.0"
        - name: REDIS_URL
          value: "redis://redis-service:6379"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: ml-model-service
spec:
  selector:
    app: ml-model-server
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ml-model-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ml-model-server
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

## üîÆ Future Enhancements

- AutoML integration for automated model training
- Advanced explainability features (SHAP, LIME)
- Multi-cloud deployment support
- Real-time feature stores
- Advanced security and compliance features
- Integration with popular ML frameworks

## üë®‚Äçüíª Author

**RAGULA SHIVA SHANKAR**
- GitHub: [@shivaragula](https://github.com/shivaragula)
- LinkedIn: [shiva-shankar-ragula](https://www.linkedin.com/in/shiva-shankar-ragula/)
- Email: shiva.shankar4997@gmail.com

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.